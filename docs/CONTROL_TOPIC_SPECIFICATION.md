# Control í† í”½ ê¸°ë°˜ ì‘ì—… ì™„ë£Œ ê´€ë¦¬ ì‹œìŠ¤í…œ ëª…ì„¸ì„œ

## ê°œìš”

Airflowì—ì„œ ë¦¬ë·° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë³„ ì™„ë£Œ ìƒíƒœë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•œ Control í† í”½ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. ë‹¨ì¼ í† í”½ì„ í†µí•´ Collection â†’ Transform â†’ Analysis â†’ Aggregation ë‹¨ê³„ì˜ ì™„ë£Œ ë° ì‹¤íŒ¨ ìƒíƒœë¥¼ ì¶”ì í•©ë‹ˆë‹¤.

## í† í”½ êµ¬ì„±

### Control í† í”½ ì •ì˜
- **í† í”½ëª…**: `job-control-topic`
- **íŒŒí‹°ì…˜**: 1 (ë©”ì‹œì§€ ìˆœì„œ ë³´ì¥)
- **ë³µì œë³¸**: 1
- **ë³´ì¡´ ì •ì±…**: delete
- **ë³´ì¡´ ê¸°ê°„**: 7ì¼ (604800000ms)

### Kafka í† í”½ ë¦¬ì†ŒìŠ¤
```yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaTopic
metadata:
  name: job-control-topic
  labels:
    strimzi.io/cluster: my-cluster
spec:
  partitions: 1
  replicas: 1
  config:
    cleanup.policy: delete
    retention.ms: 604800000
    segment.bytes: 1073741824
```

## ë©”ì‹œì§€ ìŠ¤í‚¤ë§ˆ

### ê¸°ë³¸ ë©”ì‹œì§€ êµ¬ì¡°
```json
{
  "job_id": "string",
  "step": "string",
  "status": "string",
  "expected_count": "integer",
  "actual_count": "integer",
  "timestamp": "string",
  "error_message": "string",
  "metadata": {
    "processing_time": "number",
    "worker_id": "string",
    "batch_id": "string",
    "server_info": "string"
  }
}
```

### í•„ë“œ ìƒì„¸ ì„¤ëª…

| í•„ë“œ | íƒ€ì… | í•„ìˆ˜ | ì„¤ëª… |
|------|------|------|------|
| `job_id` | string | âœ… | ì‘ì—… ì‹ë³„ì |
| `step` | string | âœ… | ì²˜ë¦¬ ë‹¨ê³„ ("collection", "transform", "analysis", "aggregation") |
| `status` | string | âœ… | ìƒíƒœ ("done", "failed", "timeout") |
| `expected_count` | integer | âœ… | ì˜ˆìƒ ì²˜ë¦¬ ìˆ˜ëŸ‰ |
| `actual_count` | integer | âœ… | ì‹¤ì œ ì²˜ë¦¬ ìˆ˜ëŸ‰ |
| `timestamp` | string | âœ… | ì™„ë£Œ/ì‹¤íŒ¨ ì‹œì  (ISO 8601 í˜•ì‹) |
| `error_message` | string | âŒ | ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë©”ì‹œì§€ |
| `metadata` | object | âŒ | ë‹¨ê³„ë³„ ì¶”ê°€ ì •ë³´ |

### ë©”ì‹œì§€ ì˜ˆì‹œ

#### Collection ì™„ë£Œ
```json
{
  "job_id": "job-2024-001",
  "step": "collection",
  "status": "done",
  "expected_count": 1000,
  "actual_count": 1000,
  "timestamp": "2024-01-15T19:30:00+09:00",
  "metadata": {
    "processing_time": 120.5,
    "worker_id": "collector-001",
    "batch_id": "batch-001",
    "server_info": "data-collector-v1.2.0"
  }
}
```

#### Transform ì™„ë£Œ
```json
{
  "job_id": "job-2024-001",
  "step": "transform",
  "status": "done",
  "expected_count": 1000,
  "actual_count": 1000,
  "timestamp": "2024-01-15T19:35:00+09:00",
  "metadata": {
    "processing_time": 45.2,
    "worker_id": "spark-worker-001",
    "batch_id": "transform-batch-001",
    "server_info": "spark-streaming-v3.4.0"
  }
}
```

#### Analysis ì‹¤íŒ¨
```json
{
  "job_id": "job-2024-001",
  "step": "analysis",
  "status": "failed",
  "expected_count": 1000,
  "actual_count": 850,
  "timestamp": "2024-01-15T19:40:00+09:00",
  "error_message": "LLM API timeout after 3 retries",
  "metadata": {
    "processing_time": 1800.0,
    "worker_id": "llm-worker-001",
    "batch_id": "analysis-batch-001",
    "server_info": "llm-api-v2.1.0"
  }
}
```

#### Aggregation ì™„ë£Œ
```json
{
  "job_id": "job-2024-001",
  "step": "aggregation",
  "status": "done",
  "expected_count": 1000,
  "actual_count": 1000,
  "timestamp": "2024-01-15T19:45:00+09:00",
  "metadata": {
    "processing_time": 12.8,
    "worker_id": "kafka-streams-001",
    "batch_id": "agg-batch-001",
    "server_info": "kafka-streams-v3.7.0"
  }
}
```

## ì²˜ë¦¬ ë‹¨ê³„ë³„ ë¡œì§

### 1. Collection ë‹¨ê³„
- **íŠ¸ë¦¬ê±°**: ë°ì´í„° ìˆ˜ì§‘ ì„œë²„ì—ì„œ ìˆ˜ì§‘ ì™„ë£Œ ì‹œ
- **ë°œí–‰ì**: Data Collection Service
- **ì™„ë£Œ ì¡°ê±´**: ìˆ˜ì§‘ëœ ë¦¬ë·° ìˆ˜ = expected_count
- **íƒ€ì„ì•„ì›ƒ**: 60ë¶„

### 2. Transform ë‹¨ê³„
- **íŠ¸ë¦¬ê±°**: Transform í† í”½ì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹  ì‹œ
- **ë°œí–‰ì**: Kafka Streams (KTable Join íŒ¨í„´)
- **ì™„ë£Œ ì¡°ê±´**: Transform í† í”½ì˜ job_idë³„ ì‹¤ì œ ë¦¬ë·° ìˆ˜ = Collection ë©”ì‹œì§€ì˜ expected_count
- **íƒ€ì„ì•„ì›ƒ**: 30ë¶„
- **í•µì‹¬ ë¡œì§**: KTable Joinì„ í†µí•œ ë™ì  ì™„ë£Œ ì²´í¬

### 3. Analysis ë‹¨ê³„
- **íŠ¸ë¦¬ê±°**: Analysis í† í”½ì—ì„œ ë©”ì‹œì§€ ìˆ˜ì‹  ì‹œ
- **ë°œí–‰ì**: Kafka Streams (KTable Join íŒ¨í„´)
- **ì™„ë£Œ ì¡°ê±´**: Analysis í† í”½ì˜ job_idë³„ ì‹¤ì œ ê²°ê³¼ ìˆ˜ = Collection ë©”ì‹œì§€ì˜ expected_count
- **íƒ€ì„ì•„ì›ƒ**: 45ë¶„
- **í•µì‹¬ ë¡œì§**: Transformê³¼ ë™ì¼í•œ KTable Join íŒ¨í„´ ì‚¬ìš©

### 4. Aggregation ë‹¨ê³„
- **íŠ¸ë¦¬ê±°**: Kafka Streamsì—ì„œ ì§‘ê³„ ì™„ë£Œ ì‹œ
- **ë°œí–‰ì**: Kafka Streams (ì§‘ê³„ ì²˜ë¦¬ ì™„ë£Œ ì‹œ)
- **ì™„ë£Œ ì¡°ê±´**: ëª¨ë“  ì§‘ê³„ ì²˜ë¦¬ ì™„ë£Œ
- **íƒ€ì„ì•„ì›ƒ**: 15ë¶„

## Kafka Streams êµ¬í˜„

### Control ë©”ì‹œì§€ ë°œí–‰ ë¡œì§
```java
public class ControlTopicManager {
    private static final ObjectMapper MAPPER = new ObjectMapper();
    private final KStream<String, String> controlStream;
    
    /**
     * í•œêµ­ ì‹œê°„(KST)ìœ¼ë¡œ í˜„ì¬ ì‹œê°„ì„ ISO 8601 í˜•ì‹ìœ¼ë¡œ ë°˜í™˜
     */
    private static String getCurrentTimeInKST() {
        ZoneId kstZone = ZoneId.of("Asia/Seoul");
        ZonedDateTime now = ZonedDateTime.now(kstZone);
        return now.format(DateTimeFormatter.ISO_OFFSET_DATE_TIME);
    }
    
    public void publishCollectionDone(String jobId, int expectedCount, int actualCount) {
        ObjectNode message = createControlMessage(jobId, "collection", "done", expectedCount, actualCount);
        controlStream.to("job-control-topic", Produced.with(Serdes.String(), Serdes.String()));
    }
    
    public void publishTransformDone(String jobId, int expectedCount, int actualCount) {
        ObjectNode message = createControlMessage(jobId, "transform", "done", expectedCount, actualCount);
        controlStream.to("job-control-topic", Produced.with(Serdes.String(), Serdes.String()));
    }
    
    public void publishAnalysisDone(String jobId, int expectedCount, int actualCount) {
        ObjectNode message = createControlMessage(jobId, "analysis", "done", expectedCount, actualCount);
        controlStream.to("job-control-topic", Produced.with(Serdes.String(), Serdes.String()));
    }
    
    public void publishAggregationDone(String jobId, int expectedCount, int actualCount) {
        ObjectNode message = createControlMessage(jobId, "aggregation", "done", expectedCount, actualCount);
        controlStream.to("job-control-topic", Produced.with(Serdes.String(), Serdes.String()));
    }
    
    public void publishFailure(String jobId, String step, String errorMessage, int expectedCount, int actualCount) {
        ObjectNode message = createControlMessage(jobId, step, "failed", expectedCount, actualCount);
        message.put("error_message", errorMessage);
        controlStream.to("job-control-topic", Produced.with(Serdes.String(), Serdes.String()));
    }
    
    private ObjectNode createControlMessage(String jobId, String step, String status, int expectedCount, int actualCount) {
        ObjectNode message = MAPPER.createObjectNode();
        message.put("job_id", jobId);
        message.put("step", step);
        message.put("status", status);
        message.put("expected_count", expectedCount);
        message.put("actual_count", actualCount);
        message.put("timestamp", getCurrentTimeInKST());
        
        ObjectNode metadata = MAPPER.createObjectNode();
        metadata.put("processing_time", 0.0);
        metadata.put("worker_id", "kafka-streams-worker");
        metadata.put("batch_id", "batch-" + System.currentTimeMillis());
        metadata.put("server_info", "kafka-streams-enhanced-v1.0.0");
        message.set("metadata", metadata);
        
        return message;
    }
}
```

### í† í´ë¡œì§€ í†µí•© (KTable Join íŒ¨í„´)
```java
public class EnhancedReviewAggregator {
    private ControlTopicManager controlManager;
    
    public void buildTopology(StreamsBuilder builder) {
        // ê¸°ì¡´ í† í´ë¡œì§€...
        
        // Control í† í”½ ëª¨ë‹ˆí„°ë§
        KStream<String, String> controlStream = builder.stream("job-control-topic");
        
        // Collection ë©”ì‹œì§€ì—ì„œ expected_count ì¶”ì¶œí•˜ì—¬ KTable ìƒì„±
        KTable<String, Integer> expectedCounts = controlStream
            .filter((key, value) -> {
                try {
                    ObjectNode message = MAPPER.readValue(value, ObjectNode.class);
                    return "collection".equals(message.get("step").asText()) && 
                           "done".equals(message.get("status").asText());
                } catch (Exception e) {
                    return false;
                }
            })
            .map((key, value) -> {
                try {
                    ObjectNode message = MAPPER.readValue(value, ObjectNode.class);
                    String jobId = message.get("job_id").asText();
                    int expectedCount = message.get("expected_count").asInt();
                    return new KeyValue<>(jobId, expectedCount);
                } catch (Exception e) {
                    return new KeyValue<>(key, 0);
                }
            })
            .groupByKey()
            .aggregate(() -> 0, (jobId, value, expectedCount) -> expectedCount,
                       Materialized.as("expected-count-store"));
        
        // Transform ë©”ì‹œì§€ì—ì„œ ì‹¤ì œ ë¦¬ë·° ìˆ˜ ì§‘ê³„í•˜ì—¬ KTable ìƒì„±
        KTable<String, Long> transformCounts = transformStream
            .map((key, value) -> {
                try {
                    ObjectNode message = MAPPER.readValue(value, ObjectNode.class);
                    String jobId = message.get("job_id").asText();
                    ArrayNode reviews = (ArrayNode) message.get("reviews");
                    int reviewCount = reviews.size();
                    return new KeyValue<>(jobId, (long) reviewCount);
                } catch (Exception e) {
                    return new KeyValue<>(key, 0L);
                }
            })
            .groupByKey(Grouped.with(Serdes.String(), Serdes.Long()))
            .aggregate(() -> 0L, (jobId, count, total) -> total + count,
                       Materialized.as("transform-count-table"));
        
        // KTable Joinìœ¼ë¡œ ë™ì  ì™„ë£Œ ì²´í¬
        KTable<String, String> transformComplete = transformCounts.join(
            expectedCounts,
            (actualCount, expectedCount) -> {
                boolean isComplete = actualCount >= expectedCount;
                return String.format("%d|%d|%s", actualCount, expectedCount, isComplete);
            }
        );
        
        // ì™„ë£Œëœ Transform ì‘ì—…ì— ëŒ€í•´ Control ë©”ì‹œì§€ ë°œí–‰
        transformComplete
            .filter((jobId, result) -> {
                String[] parts = result.split("\\|");
                return Boolean.parseBoolean(parts[2]); // isComplete
            })
            .toStream()
            .map((jobId, result) -> {
                String[] parts = result.split("\\|");
                long actualCount = Long.parseLong(parts[0]);
                int expectedCount = Integer.parseInt(parts[1]);
                
                ObjectNode message = MAPPER.createObjectNode();
                message.put("job_id", jobId);
                message.put("step", "transform");
                message.put("status", "done");
                message.put("expected_count", expectedCount);
                message.put("actual_count", (int) actualCount);
                message.put("timestamp", getCurrentTimeInKST());
                
                ObjectNode metadata = MAPPER.createObjectNode();
                metadata.put("processing_time", 0.0);
                metadata.put("worker_id", "kafka-streams-enhanced-worker");
                metadata.put("batch_id", "batch-" + System.currentTimeMillis());
                metadata.put("server_info", "kafka-streams-enhanced-v1.0.0");
                message.set("metadata", metadata);
                
                try {
                    return new KeyValue<>(jobId + "|transform", MAPPER.writeValueAsString(message));
                } catch (Exception e) {
                    return new KeyValue<>(jobId + "|transform", "{}");
                }
            })
            .to("job-control-topic", Produced.with(Serdes.String(), Serdes.String()));
            
        // Analysis ì™„ë£Œ ëª¨ë‹ˆí„°ë§ (ë™ì¼í•œ íŒ¨í„´)
        KTable<String, Long> analysisCounts = analysisStream
            .map((key, value) -> {
                try {
                    ObjectNode message = MAPPER.readValue(value, ObjectNode.class);
                    String jobId = message.get("job_id").asText();
                    ArrayNode results = (ArrayNode) message.get("results");
                    int resultCount = results.size();
                    return new KeyValue<>(jobId, (long) resultCount);
                } catch (Exception e) {
                    return new KeyValue<>(key, 0L);
                }
            })
            .groupByKey(Grouped.with(Serdes.String(), Serdes.Long()))
            .aggregate(() -> 0L, (jobId, count, total) -> total + count,
                       Materialized.as("analysis-count-table"));
        
        // Analysis ì™„ë£Œ ì²´í¬ (Transformê³¼ ë™ì¼í•œ íŒ¨í„´)
        KTable<String, String> analysisComplete = analysisCounts.join(
            expectedCounts,
            (actualCount, expectedCount) -> {
                boolean isComplete = actualCount >= expectedCount;
                return String.format("%d|%d|%s", actualCount, expectedCount, isComplete);
            }
        );
        
        analysisComplete
            .filter((jobId, result) -> {
                String[] parts = result.split("\\|");
                return Boolean.parseBoolean(parts[2]);
            })
            .toStream()
            .map((jobId, result) -> {
                String[] parts = result.split("\\|");
                long actualCount = Long.parseLong(parts[0]);
                int expectedCount = Integer.parseInt(parts[1]);
                
                ObjectNode message = MAPPER.createObjectNode();
                message.put("job_id", jobId);
                message.put("step", "analysis");
                message.put("status", "done");
                message.put("expected_count", expectedCount);
                message.put("actual_count", (int) actualCount);
                message.put("timestamp", getCurrentTimeInKST());
                
                ObjectNode metadata = MAPPER.createObjectNode();
                metadata.put("processing_time", 0.0);
                metadata.put("worker_id", "kafka-streams-enhanced-worker");
                metadata.put("batch_id", "batch-" + System.currentTimeMillis());
                metadata.put("server_info", "kafka-streams-enhanced-v1.0.0");
                message.set("metadata", metadata);
                
                try {
                    return new KeyValue<>(jobId + "|analysis", MAPPER.writeValueAsString(message));
                } catch (Exception e) {
                    return new KeyValue<>(jobId + "|analysis", "{}");
                }
            })
            .to("job-control-topic", Produced.with(Serdes.String(), Serdes.String()));
    }
}
```

## Airflow ì—°ë™

### KafkaSensor ì„¤ì •
```python
from airflow import DAG
from airflow.providers.apache.kafka.sensors.kafka import KafkaSensor
from airflow.operators.dummy import DummyOperator
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import json

def check_job_completion(job_id, step, status='done'):
    """íŠ¹ì • jobì˜ íŠ¹ì • ë‹¨ê³„ ì™„ë£Œë¥¼ í™•ì¸í•˜ëŠ” í•„í„° í•¨ìˆ˜"""
    def message_filter(message):
        try:
            data = json.loads(message.value)
            return (data['job_id'] == job_id and 
                   data['step'] == step and 
                   data['status'] == status)
        except (json.JSONDecodeError, KeyError):
            return False
    return message_filter

def check_job_failure(job_id, step):
    """íŠ¹ì • jobì˜ íŠ¹ì • ë‹¨ê³„ ì‹¤íŒ¨ë¥¼ í™•ì¸í•˜ëŠ” í•„í„° í•¨ìˆ˜"""
    def message_filter(message):
        try:
            data = json.loads(message.value)
            return (data['job_id'] == job_id and 
                   data['step'] == step and 
                   data['status'] == 'failed')
        except (json.JSONDecodeError, KeyError):
            return False
    return message_filter

def handle_failure(context):
    """ì‹¤íŒ¨ ì²˜ë¦¬ í•¨ìˆ˜"""
    job_id = context['dag_run'].conf.get('job_id')
    step = context['task_instance'].task_id.replace('wait_', '')
    print(f"Job {job_id} failed at {step} step")
    # ì•Œë¦¼ ë¡œì§ ì¶”ê°€ ê°€ëŠ¥

# DAG ì •ì˜
default_args = {
    'owner': 'data-team',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

with DAG('review_processing_monitor',
         default_args=default_args,
         description='ë¦¬ë·° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ëª¨ë‹ˆí„°ë§',
         schedule_interval=None,
         catchup=False) as dag:
    
    # Collection ì™„ë£Œ ëŒ€ê¸°
    wait_collection = KafkaSensor(
        task_id='wait_collection',
        kafka_conn_id='kafka_default',
        topic='job-control-topic',
        message_filter=check_job_completion('{{ dag_run.conf.job_id }}', 'collection'),
        timeout=3600,  # 1ì‹œê°„ íƒ€ì„ì•„ì›ƒ
        poke_interval=30,  # 30ì´ˆë§ˆë‹¤ ì²´í¬
        on_failure_callback=handle_failure
    )
    
    # Transform ì™„ë£Œ ëŒ€ê¸°
    wait_transform = KafkaSensor(
        task_id='wait_transform',
        kafka_conn_id='kafka_default',
        topic='job-control-topic',
        message_filter=check_job_completion('{{ dag_run.conf.job_id }}', 'transform'),
        timeout=1800,  # 30ë¶„ íƒ€ì„ì•„ì›ƒ
        poke_interval=30,
        on_failure_callback=handle_failure
    )
    
    # Analysis ì™„ë£Œ ëŒ€ê¸°
    wait_analysis = KafkaSensor(
        task_id='wait_analysis',
        kafka_conn_id='kafka_default',
        topic='job-control-topic',
        message_filter=check_job_completion('{{ dag_run.conf.job_id }}', 'analysis'),
        timeout=2700,  # 45ë¶„ íƒ€ì„ì•„ì›ƒ
        poke_interval=30,
        on_failure_callback=handle_failure
    )
    
    # Aggregation ì™„ë£Œ ëŒ€ê¸°
    wait_aggregation = KafkaSensor(
        task_id='wait_aggregation',
        kafka_conn_id='kafka_default',
        topic='job-control-topic',
        message_filter=check_job_completion('{{ dag_run.conf.job_id }}', 'aggregation'),
        timeout=900,  # 15ë¶„ íƒ€ì„ì•„ì›ƒ
        poke_interval=30,
        on_failure_callback=handle_failure
    )
    
    # ì™„ë£Œ ì•Œë¦¼
    notify_completion = PythonOperator(
        task_id='notify_completion',
        python_callable=lambda: print("All processing steps completed successfully!")
    )
    
    # ì‘ì—… ìˆœì„œ ì •ì˜
    wait_collection >> wait_transform >> wait_analysis >> wait_aggregation >> notify_completion
```

### DAG ì‹¤í–‰ ë°©ë²•
```bash
# Airflow CLIë¡œ DAG ì‹¤í–‰
airflow dags trigger review_processing_monitor \
  --conf '{"job_id": "job-2024-001"}'

# ë˜ëŠ” Airflow UIì—ì„œ ìˆ˜ë™ ì‹¤í–‰ ì‹œ confì— job_id ì„¤ì •
```

## ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ì‹¤íŒ¨ ê°ì§€ ë° ì•Œë¦¼
```python
def send_slack_notification(job_id, step, error_message):
    """Slack ì•Œë¦¼ ë°œì†¡"""
    webhook_url = "https://hooks.slack.com/services/YOUR/SLACK/WEBHOOK"
    message = {
        "text": f"ğŸš¨ ë¦¬ë·° ì²˜ë¦¬ ì‹¤íŒ¨ ì•Œë¦¼",
        "attachments": [
            {
                "color": "danger",
                "fields": [
                    {"title": "Job ID", "value": job_id, "short": True},
                    {"title": "ì‹¤íŒ¨ ë‹¨ê³„", "value": step, "short": True},
                    {"title": "ì—ëŸ¬ ë©”ì‹œì§€", "value": error_message, "short": False}
                ]
            }
        ]
    }
    
    requests.post(webhook_url, json=message)

def send_email_notification(job_id, step, error_message):
    """ì´ë©”ì¼ ì•Œë¦¼ ë°œì†¡"""
    # ì´ë©”ì¼ ë°œì†¡ ë¡œì§
    pass
```

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘
```python
from prometheus_client import Counter, Histogram, Gauge

# ë©”íŠ¸ë¦­ ì •ì˜
control_messages_total = Counter('control_messages_total', 'Total control messages', ['step', 'status'])
processing_duration = Histogram('processing_duration_seconds', 'Processing duration', ['step'])
active_jobs = Gauge('active_jobs_total', 'Number of active jobs', ['step'])

def update_metrics(message_data):
    """ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
    control_messages_total.labels(
        step=message_data['step'], 
        status=message_data['status']
    ).inc()
    
    if 'processing_time' in message_data.get('metadata', {}):
        processing_duration.labels(
            step=message_data['step']
        ).observe(message_data['metadata']['processing_time'])
```

## ìš´ì˜ ê°€ì´ë“œ

### í† í”½ ê´€ë¦¬
```bash
# í† í”½ ìƒíƒœ í™•ì¸
kubectl -n kafka exec -it my-cluster-broker-0 -- bash -lc \
  "/opt/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic job-control-topic"

# ë©”ì‹œì§€ í™•ì¸
kubectl -n kafka exec -it my-cluster-broker-0 -- bash -lc \
  "/opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic job-control-topic --from-beginning --max-messages 10"

# ì»¨ìŠˆë¨¸ ê·¸ë£¹ ìƒíƒœ í™•ì¸
kubectl -n kafka exec -it my-cluster-broker-0 -- bash -lc \
  "/opt/kafka/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group airflow-monitor"
```

### ë¬¸ì œ í•´ê²°
1. **ë©”ì‹œì§€ ëˆ„ë½**: ì»¨ìŠˆë¨¸ ê·¸ë£¹ ì˜¤í”„ì…‹ í™•ì¸
2. **íƒ€ì„ì•„ì›ƒ**: ê° ë‹¨ê³„ë³„ íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¡°ì •
3. **ì¤‘ë³µ ë©”ì‹œì§€**: exactly-once ë³´ì¥ ì„¤ì • í™•ì¸
4. **ìˆœì„œ ë¬¸ì œ**: íŒŒí‹°ì…˜ 1ê°œ ì‚¬ìš©ìœ¼ë¡œ ìˆœì„œ ë³´ì¥

## ë³´ì•ˆ ê³ ë ¤ì‚¬í•­

### ì ‘ê·¼ ì œì–´
- Kafka ACL ì„¤ì •ìœ¼ë¡œ í† í”½ ì ‘ê·¼ ê¶Œí•œ ì œì–´
- Airflowì—ì„œ ì‚¬ìš©í•˜ëŠ” ì»¨ìŠˆë¨¸ ê·¸ë£¹ë³„ ê¶Œí•œ ë¶„ë¦¬

### ë°ì´í„° ë³´í˜¸
- ë¯¼ê°í•œ ì •ë³´ëŠ” metadataì— í¬í•¨í•˜ì§€ ì•ŠìŒ
- ì—ëŸ¬ ë©”ì‹œì§€ì—ì„œ ê°œì¸ì •ë³´ ì œê±°

## í™•ì¥ì„± ê³ ë ¤ì‚¬í•­

### ì„±ëŠ¥ ìµœì í™”
- íŒŒí‹°ì…˜ ìˆ˜ ì¦ê°€ ì‹œ ìˆœì„œ ë³´ì¥ ê³ ë ¤
- ì»¨ìŠˆë¨¸ ê·¸ë£¹ë³„ ë…ë¦½ì ì¸ ì˜¤í”„ì…‹ ê´€ë¦¬
- ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë©”ì‹œì§€ ì²˜ë¦¬ëŸ‰ í–¥ìƒ

### ëª¨ë‹ˆí„°ë§ í™•ì¥
- Grafana ëŒ€ì‹œë³´ë“œ ì—°ë™
- ì•Œë¦¼ ì±„ë„ í™•ì¥ (Slack, ì´ë©”ì¼, SMS)
- ë¡œê·¸ ì§‘ê³„ ë° ë¶„ì„

## ì‹¤ì œ êµ¬í˜„ í˜„í™© (2025-09-15 ì—…ë°ì´íŠ¸)

### âœ… êµ¬í˜„ ì™„ë£Œëœ ê¸°ëŠ¥
- **KTable Join íŒ¨í„´**: Collection ë©”ì‹œì§€ì˜ `expected_count`ì™€ Transform/Analysisì˜ ì‹¤ì œ ì¹´ìš´íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ë¹„êµ
- **í•œêµ­ ì‹œê°„(KST) ì ìš©**: ëª¨ë“  timestampê°€ í•œêµ­ ì‹œê°„ìœ¼ë¡œ ìƒì„±ë¨ (`Asia/Seoul` íƒ€ì„ì¡´)
- **ì‹¤ì œ ë¦¬ë·° ìˆ˜ ì§‘ê³„**: ë©”ì‹œì§€ ìˆ˜ê°€ ì•„ë‹Œ ì‹¤ì œ ë¦¬ë·° ìˆ˜ë¡œ ì •í™•í•œ ì¹´ìš´íŒ…
- **ë™ì  ì™„ë£Œ ì²´í¬**: í•˜ë“œì½”ë”© ì œê±°í•˜ê³  ëŸ°íƒ€ì„ì— `actualCount >= expectedCount` ë¹„êµ
- **íƒ€ì… ì•ˆì „ì„±**: ëª…ì‹œì  Serdes ì§€ì •ìœ¼ë¡œ `ClassCastException` ë°©ì§€

### ğŸ”§ í•µì‹¬ ê¸°ìˆ ì  ê°œì„ ì‚¬í•­
1. **KTable Join íŒ¨í„´ ë„ì…**
   ```java
   // Collection ë©”ì‹œì§€ì—ì„œ expected_count ì¶”ì¶œ
   KTable<String, Integer> expectedCounts = controlStream...
   
   // Transform ë©”ì‹œì§€ì—ì„œ ì‹¤ì œ ë¦¬ë·° ìˆ˜ ì§‘ê³„
   KTable<String, Long> transformCounts = transformStream...
   
   // ë‘ KTableì„ Joiní•˜ì—¬ ë™ì  ì™„ë£Œ ì²´í¬
   KTable<String, String> transformComplete = transformCounts.join(expectedCounts, ...);
   ```

2. **í•œêµ­ ì‹œê°„ ì ìš©**
   ```java
   private static String getCurrentTimeInKST() {
       ZoneId kstZone = ZoneId.of("Asia/Seoul");
       ZonedDateTime now = ZonedDateTime.now(kstZone);
       return now.format(DateTimeFormatter.ISO_OFFSET_DATE_TIME);
   }
   ```

3. **ì •í™•í•œ ë¦¬ë·° ìˆ˜ ì§‘ê³„**
   ```java
   // ì´ì „: groupByKey().count() - ë©”ì‹œì§€ ìˆ˜ ì§‘ê³„
   // í˜„ì¬: groupByKey().aggregate() - ì‹¤ì œ ë¦¬ë·° ìˆ˜ ì§‘ê³„
   .aggregate(() -> 0L, (jobId, count, total) -> total + count)
   ```

### ğŸ“Š í…ŒìŠ¤íŠ¸ ê²€ì¦ ì™„ë£Œ
- âœ… `expected_count=2` â†’ Transform ë©”ì‹œì§€ 2ê°œ â†’ ì™„ë£Œ
- âœ… `expected_count=3` â†’ Transform ë©”ì‹œì§€ 3ê°œ â†’ ì™„ë£Œ  
- âœ… `expected_count=5` â†’ Transform ë©”ì‹œì§€ 2ê°œ+3ê°œ â†’ ì™„ë£Œ
- âœ… í•œêµ­ ì‹œê°„(KST) ì ìš© í™•ì¸
- âœ… Control Topic ë©”ì‹œì§€ ì •ìƒ ë°œí–‰ (`expected_count`, `actual_count` ì •í™•íˆ ë°˜ì˜)

### ğŸ¯ ìš´ì˜ ìƒíƒœ
- **ìƒíƒœ**: âœ… **ëª¨ë“  í•µì‹¬ ê¸°ëŠ¥ êµ¬í˜„ ë° ê²€ì¦ ì™„ë£Œ**
- **ë°°í¬**: âœ… **í”„ë¡œë•ì…˜ í™˜ê²½ ë°°í¬ ì™„ë£Œ**
- **ëª¨ë‹ˆí„°ë§**: âœ… **ì‹¤ì‹œê°„ ë¡œê·¸ ëª¨ë‹ˆí„°ë§ ê°€ëŠ¥**
- **í™•ì¥ì„±**: âœ… **ë™ì  ì‘ì—… í¬ê¸° ì²˜ë¦¬ ê°€ëŠ¥**

ì´ Control í† í”½ ì‹œìŠ¤í…œì„ í†µí•´ Airflowì—ì„œ ë¦¬ë·° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ê° ë‹¨ê³„ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
